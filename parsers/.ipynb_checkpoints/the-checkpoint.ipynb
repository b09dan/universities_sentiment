{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen  # Library for urlopen\n",
    "from bs4 import BeautifulSoup  # Library for html parser (scraper), lxml is also nice\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('..') \n",
    "from uni_cache.cache_function import cache_function\n",
    "import pymysql\n",
    "import collections\n",
    "import mysql_credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This folder should be edited according to this project path on yours computer\n",
    "project_folder = '/home/bogdan/PycharmProjects/universities_sentiment/'\n",
    "cache_folder = project_folder + 'cache/'\n",
    "site = 'https://www.timeshighereducation.com'\n",
    "\n",
    "\n",
    "connection = pymysql.connect(\n",
    "    host=mysql_credits.db_host,\n",
    "    user=mysql_credits.db_user,\n",
    "    password=mysql_credits.db_password,\n",
    "    db=mysql_credits.db,\n",
    "    charset='utf8mb4',\n",
    "    cursorclass=pymysql.cursors.DictCursor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemming_article(article): \n",
    "    from nltk.stem import SnowballStemmer \n",
    "    for_removing = \"№#©@&%\\\\\\/=+/~^*,.;:\\\"'`“”‘’–-—_{}[]()1234567890!@#?$\" \n",
    "    lines = open(article, \"r\", encoding=\"utf8\", errors='replace').readlines() \n",
    "    stemmer = SnowballStemmer(\"russian\") \n",
    "    for line in lines: \n",
    "        line = line.replace(\"\\n\", \"\") \n",
    "        print(stemmer.stem(line)) \n",
    "#stemming_article(\"minus_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_function(site_tree):\n",
    "    # Преобразовываем файл к объектному типу библиотеки BeautifulSoup\n",
    "    site_bs = BeautifulSoup(site_tree, \"html.parser\")\n",
    "    # Ищем все вхождения ссылок с главной страницы на статьи\n",
    "    site_titles = site_bs.find_all('div', attrs={\"class\":\"views-row\"})\n",
    "    articles_data = []\n",
    "    for site_title in site_titles:\n",
    "        if site_title.find('div', attrs={\"class\":\"submitted\"}):\n",
    "            # Создаём что-то вроде ассоциативного массива\n",
    "            article_meta_data = collections.OrderedDict()\n",
    "            # Херачим в него ссылки статьи\n",
    "\n",
    "            article_meta_data['article_url'] = site + site_title.find('a', attrs={\"data-position\":\"list\"}).get('href')\n",
    "            # Херачим в него заголовки статьи\n",
    "            article_meta_data['article_title'] = re.sub(' +',' ',site_title.get_text().replace(\"\\\\n\", \"\").replace(\"\\n\", \"\")) \n",
    "            article_meta_data['article_date'] =re.sub(' +',' ', site_title.find('div', attrs={\"class\":\"submitted\"}).get_text().replace(\"\\\\n\", \"\").replace(\"\\n\", \"\")) \n",
    "            #print(site_title.get_text())\n",
    "            # Вкорячиваем этот \"ассоциативный массив\" в просто массив\n",
    "            articles_data.append(article_meta_data)\n",
    "    return articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using web page from internet...\n",
      "8\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    if i == 1:\n",
    "        site_news_section = '/student/news'\n",
    "    else:\n",
    "        site_news_section = '/student/news?page='+str(i)\n",
    "    site_tree = cache_function(site + site_news_section)\n",
    "    articles = parsing_function(site_tree)\n",
    "    print(len(articles))\n",
    "    # Здесь будем парсить все статьи,собранные с главной страницы\n",
    "    for article in articles:\n",
    "        site_tree = cache_function(article['article_url'])\n",
    "        page_article_bs = BeautifulSoup(site_tree, \"html.parser\")\n",
    "        page_article = page_article_bs.find_all('div', class_='field-item even')\n",
    "        article_content=''\n",
    "        for wrapper in page_article:\n",
    "            if wrapper.find(\"p\"):\n",
    "                article_content=article_content+wrapper.find(\"p\").get_text()\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "                # Create a new record\n",
    "                #INSERT INTO `article` (`article_title`, `article_text`, `article_url`, `article_categories`) VALUES (%s, %s, %s, 'null');\n",
    "                sql = '''\n",
    "                INSERT INTO `article` (`article_pub_date`, `article_title`, `article_text`, `article_url`, `article_rating`, `article_uni`, `uni_site_id`)\n",
    "                VALUES (%s, %s, %s,%s, %s, %s, %s);\n",
    "                '''\n",
    "                \n",
    "                #sql=\"INSERT INTO `article` (`article_pub_date`, `article_title`, `article_text`, `article_url`, `article_rating`, `article_uni`, `uni_site_id`) VALUES ('\"+article['article_date']+\"','\"+article['article_title']+ \"', '\"+article_content+\"', '\"+article['article_url']+\"', 'null', 'null',null');\"\n",
    "            \n",
    "                #ins=\n",
    "                cursor.execute(sql, (str(pd.to_datetime(article['article_date'])), article['article_title'], article_content, article['article_url'], 0,0,0))\n",
    "                #cursor.execute(sql)\n",
    "            # connection is not autocommit by default. So you must commit to save\n",
    "            # your changes.\n",
    "            connection.commit()\n",
    "        finally:\n",
    "            print('finally')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "import hashlib\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using web page from internet...\n",
      "8\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n",
      "Using web page from internet...\n",
      "finally\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    if i == 1:\n",
    "        site_news_section = '/student/blogs'\n",
    "    else:\n",
    "        site_news_section = '/student/blogs?page='+str(i)\n",
    "    site_tree = cache_function(site + site_news_section)\n",
    "    articles = parsing_function(site_tree)\n",
    "    print(len(articles))\n",
    "    # Здесь будем парсить все статьи,собранные с главной страницы\n",
    "    for article in articles:\n",
    "        site_tree = cache_function(article['article_url'])\n",
    "        page_article_bs = BeautifulSoup(site_tree, \"html.parser\")\n",
    "        page_article = page_article_bs.find_all('div', class_='field-item even')\n",
    "        article_content=''\n",
    "        for wrapper in page_article:\n",
    "            if wrapper.find(\"p\"):\n",
    "                article_content=article_content+wrapper.find(\"p\").get_text()\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "                # Create a new record\n",
    "                #INSERT INTO `article` (`article_title`, `article_text`, `article_url`, `article_categories`) VALUES (%s, %s, %s, 'null');\n",
    "                sql = '''\n",
    "                INSERT INTO `article` (`article_pub_date`, `article_title`, `article_text`, `article_url`, `article_rating`, `article_uni`, `uni_site_id`)\n",
    "                VALUES (%s, %s, %s,%s, %s, %s, %s);\n",
    "                '''\n",
    "                \n",
    "                #sql=\"INSERT INTO `article` (`article_pub_date`, `article_title`, `article_text`, `article_url`, `article_rating`, `article_uni`, `uni_site_id`) VALUES ('\"+article['article_date']+\"','\"+article['article_title']+ \"', '\"+article_content+\"', '\"+article['article_url']+\"', 'null', 'null',null');\"\n",
    "            \n",
    "                #ins=\n",
    "                cursor.execute(sql, (str(pd.to_datetime(article['article_date'])), article['article_title'], article_content, article['article_url'], 0,0,0))\n",
    "                #cursor.execute(sql)\n",
    "            # connection is not autocommit by default. So you must commit to save\n",
    "            # your changes.\n",
    "            connection.commit()\n",
    "        finally:\n",
    "            print('finally')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
